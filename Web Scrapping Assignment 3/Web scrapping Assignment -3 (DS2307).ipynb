{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af89c665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for on Amazon.in: Guitar\n",
      "An error occurred: 503 Server Error: Service Unavailable for url: https://www.amazon.in/s?k=Guitar\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon_products(search_query):\n",
    "    base_url = \"https://www.amazon.in/\"\n",
    "    search_query = search_query.replace(\" \", \"+\")\n",
    "    search_url = f\"{base_url}s?k={search_query}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        product_elements = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "        \n",
    "        if not product_elements:\n",
    "            print(\"No products found.\")\n",
    "            return\n",
    "\n",
    "        for product in product_elements:\n",
    "            title_element = product.find(\"span\", class_=\"a-text-normal\")\n",
    "            price_element = product.find(\"span\", class_=\"a-price-whole\")\n",
    "            \n",
    "            if title_element and price_element:\n",
    "                title = title_element.text.strip()\n",
    "                price = price_element.text.strip()\n",
    "                print(f\"Title: {title}\")\n",
    "                print(f\"Price: â‚¹{price}\")\n",
    "                print(\"-\" * 30)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    search_amazon_products(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb9149e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for on Amazon.in: guitar\n",
      "An error occurred: 503 Server Error: Service Unavailable for url: https://www.amazon.in/s?k=guitar&page=1\n",
      "No data to save.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(search_query, num_pages=3):\n",
    "    base_url = \"https://www.amazon.in/\"\n",
    "    search_query = search_query.replace(\" \", \"+\")\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"Brand Name\": [],\n",
    "        \"Name of the Product\": [],\n",
    "        \"Price\": [],\n",
    "        \"Return/Exchange\": [],\n",
    "        \"Expected Delivery\": [],\n",
    "        \"Availability\": [],\n",
    "        \"Product URL\": [],\n",
    "    }\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        search_url = f\"{base_url}s?k={search_query}&page={page}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            product_elements = soup.find_all(\"div\", class_=\"s-result-item\")\n",
    "            \n",
    "            if not product_elements:\n",
    "                print(\"No products found.\")\n",
    "                break\n",
    "\n",
    "            for product in product_elements:\n",
    "                title_element = product.find(\"span\", class_=\"a-text-normal\")\n",
    "                price_element = product.find(\"span\", class_=\"a-price-whole\")\n",
    "                return_exchange_element = product.find(\"div\", class_=\"a-row a-size-small\")\n",
    "                expected_delivery_element = product.find(\"span\", class_=\"a-text-bold\")\n",
    "                availability_element = product.find(\"span\", class_=\"a-declarative\")\n",
    "                \n",
    "                if title_element and price_element:\n",
    "                    title = title_element.text.strip()\n",
    "                    price = price_element.text.strip()\n",
    "                    brand = title.split()[0]  # Assume brand name is the first word in the title\n",
    "                    return_exchange = return_exchange_element.text.strip() if return_exchange_element else \"-\"\n",
    "                    expected_delivery = expected_delivery_element.text.strip() if expected_delivery_element else \"-\"\n",
    "                    availability = availability_element.text.strip() if availability_element else \"-\"\n",
    "                    product_url = base_url + title_element.parent['href']\n",
    "\n",
    "                    data[\"Brand Name\"].append(brand)\n",
    "                    data[\"Name of the Product\"].append(title)\n",
    "                    data[\"Price\"].append(price)\n",
    "                    data[\"Return/Exchange\"].append(return_exchange)\n",
    "                    data[\"Expected Delivery\"].append(expected_delivery)\n",
    "                    data[\"Availability\"].append(availability)\n",
    "                    data[\"Product URL\"].append(product_url)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search for on Amazon.in: \")\n",
    "    num_pages = 3  # You can change this number to scrape more or fewer pages\n",
    "    df = scrape_product_details(user_input, num_pages)\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Save the data to a CSV file\n",
    "        df.to_csv(\"amazon_products.csv\", index=False)\n",
    "        print(\"Data saved to 'amazon_products.csv'\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86818015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred for 'fruits': 'WebDriver' object has no attribute 'find_element_by_name'\n",
      "An error occurred for 'cars': 'WebDriver' object has no attribute 'find_element_by_name'\n",
      "An error occurred for 'Machine Learning': 'WebDriver' object has no attribute 'find_element_by_name'\n",
      "An error occurred for 'Guitar': 'WebDriver' object has no attribute 'find_element_by_name'\n",
      "An error occurred for 'Cakes': 'WebDriver' object has no attribute 'find_element_by_name'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "def scrape_google_images(keywords, num_images=10):\n",
    "    # Initialize the Chrome WebDriver (ensure you have chromedriver.exe in your PATH)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    for keyword in keywords:\n",
    "        try:\n",
    "            # Open Google Images\n",
    "            driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "            # Locate the search bar element and input the keyword\n",
    "            search_bar = driver.find_element_by_name(\"q\")\n",
    "            search_bar.clear()\n",
    "            search_bar.send_keys(keyword)\n",
    "            search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "            # Scroll down the page to load more images (you may need to adjust this for more results)\n",
    "            for _ in range(num_images // 20):  # Scroll approximately for 20 images per scroll\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(2)  # Wait for the new images to load\n",
    "\n",
    "            # Find and click on each image to open in a new tab and retrieve its URL\n",
    "            image_elements = driver.find_elements_by_css_selector(\".rg_i\")\n",
    "            image_urls = []\n",
    "            for i, image_element in enumerate(image_elements[:num_images]):\n",
    "                image_element.click()\n",
    "                time.sleep(2)  # Wait for the image to open in a new tab\n",
    "                tabs = driver.window_handles\n",
    "                driver.switch_to.window(tabs[1])\n",
    "                image_url = driver.current_url\n",
    "                image_urls.append(image_url)\n",
    "                driver.close()\n",
    "                driver.switch_to.window(tabs[0])\n",
    "\n",
    "            # Print the URLs of the scraped images\n",
    "            print(f\"Images for '{keyword}':\")\n",
    "            for i, image_url in enumerate(image_urls, start=1):\n",
    "                print(f\"{i}. {image_url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for '{keyword}': {str(e)}\")\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "    scrape_google_images(keywords, num_images=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd00609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the smartphone you want to search for on Flipkart: Oneplus Nord\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     64\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the smartphone you want to search for on Flipkart: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_flipkart_smartphones\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# Save the data to a CSV file\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflipkart_smartphones.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 54\u001b[0m, in \u001b[0;36mscrape_flipkart_smartphones\u001b[1;34m(search_query)\u001b[0m\n\u001b[0;32m     51\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct URL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(product_url)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Initialize other details with \"-\" in case they are not found\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m other_details_dict \u001b[38;5;241m=\u001b[39m {detail\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]: detail\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m detail \u001b[38;5;129;01min\u001b[39;00m other_details}\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrand\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColour\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimary Camera\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecondary Camera\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisplay Size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBattery Capacity\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     56\u001b[0m     data[key]\u001b[38;5;241m.\u001b[39mappend(other_details_dict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[5], line 54\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProduct URL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(product_url)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Initialize other details with \"-\" in case they are not found\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m other_details_dict \u001b[38;5;241m=\u001b[39m {detail\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]: \u001b[43mdetail\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m detail \u001b[38;5;129;01min\u001b[39;00m other_details}\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBrand\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColour\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrimary Camera\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecondary Camera\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisplay Size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBattery Capacity\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     56\u001b[0m     data[key]\u001b[38;5;241m.\u001b[39mappend(other_details_dict\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    base_url = f\"https://www.flipkart.com/search?q={search_query}\"\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"Brand Name\": [],\n",
    "        \"Smartphone Name\": [],\n",
    "        \"Colour\": [],\n",
    "        \"RAM\": [],\n",
    "        \"Storage(ROM)\": [],\n",
    "        \"Primary Camera\": [],\n",
    "        \"Secondary Camera\": [],\n",
    "        \"Display Size\": [],\n",
    "        \"Battery Capacity\": [],\n",
    "        \"Price\": [],\n",
    "        \"Product URL\": [],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        product_elements = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "        \n",
    "        if not product_elements:\n",
    "            print(\"No products found.\")\n",
    "            return\n",
    "\n",
    "        for product in product_elements:\n",
    "            title_element = product.find(\"div\", class_=\"_4rR01T\")\n",
    "            price_element = product.find(\"div\", class_=\"_30jeq3\")\n",
    "            url_element = product.find(\"a\", class_=\"_1fQZEK\")\n",
    "            other_details = product.find_all(\"li\", class_=\"rgWa7D\")\n",
    "\n",
    "            if title_element and price_element and url_element:\n",
    "                title = title_element.text.strip()\n",
    "                price = price_element.text.strip()\n",
    "                product_url = \"https://www.flipkart.com\" + url_element[\"href\"]\n",
    "\n",
    "                data[\"Smartphone Name\"].append(title)\n",
    "                data[\"Price\"].append(price)\n",
    "                data[\"Product URL\"].append(product_url)\n",
    "\n",
    "                # Initialize other details with \"-\" in case they are not found\n",
    "                other_details_dict = {detail.get_text(strip=True).split(\":\")[0]: detail.get_text(strip=True).split(\":\")[1] for detail in other_details}\n",
    "                for key in [\"Brand\", \"Colour\", \"RAM\", \"ROM\", \"Primary Camera\", \"Secondary Camera\", \"Display Size\", \"Battery Capacity\"]:\n",
    "                    data[key].append(other_details_dict.get(key, \"-\"))\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    df = scrape_flipkart_smartphones(user_input)\n",
    "    \n",
    "    if not df.empty:\n",
    "        # Save the data to a CSV file\n",
    "        df.to_csv(\"flipkart_smartphones.csv\", index=False)\n",
    "        print(\"Data saved to 'flipkart_smartphones.csv'\")\n",
    "    else:\n",
    "        print(\"No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d1688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the city: Dhaka\n",
      "Coordinates for Dhaka not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    base_url = \"https://www.google.com/maps/place/\"\n",
    "\n",
    "    # Replace spaces with '+' in the city name for the URL\n",
    "    search_query = city_name.replace(\" \", \"+\")\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the Google Maps URL\n",
    "        response = requests.get(f\"{base_url}{search_query}\")\n",
    "\n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find the latitude and longitude from the page\n",
    "        coordinates = soup.find(\"meta\", attrs={\"itemprop\": \"geo\"})\n",
    "        if coordinates:\n",
    "            latitude = coordinates[\"content\"].split(\",\")[0]\n",
    "            longitude = coordinates[\"content\"].split(\",\")[1]\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the name of the city: \")\n",
    "    coordinates = get_coordinates(city_name)\n",
    "\n",
    "    if coordinates:\n",
    "        latitude, longitude = coordinates\n",
    "        print(f\"Coordinates for {city_name}:\")\n",
    "        print(f\"Latitude: {latitude}\")\n",
    "        print(f\"Longitude: {longitude}\")\n",
    "    else:\n",
    "        print(f\"Coordinates for {city_name} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7b559e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3494213420.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 33\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"=\" * 50)\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_digit_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the container that holds the laptop details\n",
    "        laptops_container = soup.find(\"div\", class_=\"TopNumbeHeading\")\n",
    "\n",
    "        # Initialize lists to store laptop details\n",
    "        laptop_names = []\n",
    "        laptop_prices = []\n",
    "\n",
    "        # Extract laptop details\n",
    "        for laptop in laptops_container.find_all(\"div\", class_=\"TopNumbeHeadingmid\"):\n",
    "            name = laptop.find(\"div\", class_=\"TopNumbeHeadingright\").text.strip()\n",
    "            price = laptop.find(\"div\", class_=\"TopNumbeHeadingrightnum\").text.strip()\n",
    "            laptop_names.append(name)\n",
    "            laptop_prices.append(price)\n",
    "\n",
    "        # Print the scraped data\n",
    "        for i in range(len(laptop_names)):\n",
    "            print(f\"Laptop Name: {laptop_names[i]}\")\n",
    "            print(f\"Price: {laptop_prices[i]}\")\n",
    "            print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3a5d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement googleapipythonclient (from versions: none)\n",
      "ERROR: No matching distribution found for googleapipythonclient\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade googleapipythonclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287827e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d59395",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m     \u001b[43mscrape_forbes_billionaires\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m, in \u001b[0;36mscrape_forbes_billionaires\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m industries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Extract billionaire details\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbillionaires_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable-row\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     30\u001b[0m     rank \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     31\u001b[0m     name \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersonName\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the Forbes billionaire page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the container that holds the billionaire details\n",
    "        billionaires_container = soup.find(\"div\", class_=\"table-body\")\n",
    "\n",
    "        # Initialize lists to store billionaire details\n",
    "        ranks = []\n",
    "        names = []\n",
    "        net_worths = []\n",
    "        ages = []\n",
    "        citizenships = []\n",
    "        sources = []\n",
    "        industries = []\n",
    "\n",
    "        # Extract billionaire details\n",
    "        for row in billionaires_container.find_all(\"div\", class_=\"table-row\"):\n",
    "            rank = row.find(\"div\", class_=\"rank\").text.strip()\n",
    "            name = row.find(\"div\", class_=\"personName\").text.strip()\n",
    "            net_worth = row.find(\"div\", class_=\"netWorth\").text.strip()\n",
    "            age = row.find(\"div\", class_=\"age\").text.strip()\n",
    "            citizenship = row.find(\"div\", class_=\"countryOfCitizenship\").text.strip()\n",
    "            source = row.find(\"div\", class_=\"source\").text.strip()\n",
    "            industry = row.find(\"div\", class_=\"category\").text.strip()\n",
    "\n",
    "            ranks.append(rank)\n",
    "            names.append(name)\n",
    "            net_worths.append(net_worth)\n",
    "            ages.append(age)\n",
    "            citizenships.append(citizenship)\n",
    "            sources.append(source)\n",
    "            industries.append(industry)\n",
    "\n",
    "        # Create a DataFrame to store the scraped data\n",
    "        data = {\n",
    "            \"Rank\": ranks,\n",
    "            \"Name\": names,\n",
    "            \"Net Worth\": net_worths,\n",
    "            \"Age\": ages,\n",
    "            \"Citizenship\": citizenships,\n",
    "            \"Source\": sources,\n",
    "            \"Industry\": industries\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Print or save the scraped data as needed\n",
    "        print(df)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_forbes_billionaires()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1701639",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googleapiclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogleapiclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set up YouTube Data API credentials\u001b[39;00m\n\u001b[0;32m      5\u001b[0m api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYouTube Data API\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "\n",
    "# Set up YouTube Data API credentials\n",
    "api_key = \"YouTube Data API\"  \n",
    "\n",
    "# Initialize the YouTube Data API client\n",
    "youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "def get_video_comments(video_id, max_results=500):\n",
    "    comments = []\n",
    "    nextPageToken = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        try:\n",
    "            results = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(100, max_results - len(comments)),\n",
    "                textFormat=\"plainText\",\n",
    "                pageToken=nextPageToken\n",
    "            ).execute()\n",
    "\n",
    "            for item in results[\"items\"]:\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                comments.append({\n",
    "                    \"text\": comment[\"textDisplay\"],\n",
    "                    \"upvotes\": comment.get(\"likeCount\", 0),\n",
    "                    \"time\": comment[\"publishedAt\"]\n",
    "                })\n",
    "\n",
    "            nextPageToken = results.get(\"nextPageToken\")\n",
    "\n",
    "            if not nextPageToken:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_id = input(\"Enter the YouTube video ID: \")\n",
    "    comments = get_video_comments(video_id)\n",
    "\n",
    "    print(f\"Total Comments: {len(comments)}\")\n",
    "\n",
    "    # Print the first 10 comments as an example\n",
    "    for i, comment in enumerate(comments[:10], start=1):\n",
    "        print(f\"Comment {i}:\")\n",
    "        print(f\"Text: {comment['text']}\")\n",
    "        print(f\"Upvotes: {comment['upvotes']}\")\n",
    "        print(f\"Time: {comment['time']}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10b10c43",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     68\u001b[0m     location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLondon\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mscrape_hostel_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 30\u001b[0m, in \u001b[0;36mscrape_hostel_data\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m     27\u001b[0m descriptions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Extract hostel details\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hostel \u001b[38;5;129;01min\u001b[39;00m \u001b[43mhostels_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfabresult\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     31\u001b[0m     hostel_name \u001b[38;5;241m=\u001b[39m hostel\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     32\u001b[0m     distance \u001b[38;5;241m=\u001b[39m hostel\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostel_data(location):\n",
    "    base_url = f\"https://www.hostelworld.com/hostels/{location}\"\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the hostelworld website\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the container that holds hostel information\n",
    "        hostels_container = soup.find(\"div\", class_=\"fabcontainer\")\n",
    "\n",
    "        # Initialize lists to store hostel details\n",
    "        hostel_names = []\n",
    "        distances = []\n",
    "        ratings = []\n",
    "        total_reviews = []\n",
    "        overall_reviews = []\n",
    "        privates_prices = []\n",
    "        dorms_prices = []\n",
    "        facilities = []\n",
    "        descriptions = []\n",
    "\n",
    "        # Extract hostel details\n",
    "        for hostel in hostels_container.find_all(\"div\", class_=\"fabresult\"):\n",
    "            hostel_name = hostel.find(\"h2\").text.strip()\n",
    "            distance = hostel.find(\"span\", class_=\"distance\").text.strip()\n",
    "            rating = hostel.find(\"div\", class_=\"score\").text.strip()\n",
    "            reviews = hostel.find(\"div\", class_=\"reviews\").text.strip()\n",
    "            overall_review = hostel.find(\"div\", class_=\"keyword\").text.strip()\n",
    "            private_price = hostel.find(\"div\", class_=\"price privates\").text.strip()\n",
    "            dorm_price = hostel.find(\"div\", class_=\"price dorms\").text.strip()\n",
    "            facility = [f.text.strip() for f in hostel.find_all(\"div\", class_=\"label\")]\n",
    "            description = hostel.find(\"div\", class_=\"text\").text.strip()\n",
    "\n",
    "            hostel_names.append(hostel_name)\n",
    "            distances.append(distance)\n",
    "            ratings.append(rating)\n",
    "            total_reviews.append(reviews)\n",
    "            overall_reviews.append(overall_review)\n",
    "            privates_prices.append(private_price)\n",
    "            dorms_prices.append(dorm_price)\n",
    "            facilities.append(facility)\n",
    "            descriptions.append(description)\n",
    "\n",
    "        # Print or store the scraped data as needed\n",
    "        for i in range(len(hostel_names)):\n",
    "            print(f\"Hostel Name: {hostel_names[i]}\")\n",
    "            print(f\"Distance from City Centre: {distances[i]}\")\n",
    "            print(f\"Ratings: {ratings[i]}\")\n",
    "            print(f\"Total Reviews: {total_reviews[i]}\")\n",
    "            print(f\"Overall Reviews: {overall_reviews[i]}\")\n",
    "            print(f\"Private Room Price: {privates_prices[i]}\")\n",
    "            print(f\"Dorm Room Price: {dorms_prices[i]}\")\n",
    "            print(f\"Facilities: {', '.join(facilities[i])}\")\n",
    "            print(f\"Property Description: {descriptions[i]}\")\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    location = \"London\"\n",
    "    scrape_hostel_data(location)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
