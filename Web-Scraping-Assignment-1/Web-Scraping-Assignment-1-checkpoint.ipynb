{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c44efd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Header Tags\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you knowÂ ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all header tags (h1 to h6)\n",
    "    header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    \n",
    "    # Extract the text content of the header tags\n",
    "    header_texts = [tag.get_text() for tag in header_tags]\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({'Header Tags': header_texts})\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print('Failed to retrieve the page.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043bbb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the information\n",
    "    table = soup.find('table', {'class': 'views-table'})\n",
    "    \n",
    "    # Extract the rows from the table\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    # Initialize lists to store the data\n",
    "    names = []\n",
    "    terms = []\n",
    "    \n",
    "    # Iterate through each row and extract the data\n",
    "    for row in rows[1:]:  # Skipping the header row\n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].get_text().strip()\n",
    "        term = columns[1].get_text().strip()\n",
    "        names.append(name)\n",
    "        terms.append(term)\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame({'Name': names, 'Term of Office': terms})\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print('Failed to retrieve the page.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f36afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "   Rank                Team Matches Points\n",
      "0     1      Australia\\nAUS      23    118\n",
      "1     2       Pakistan\\nPAK      20    116\n",
      "2     3          India\\nIND      36    113\n",
      "3     4     New Zealand\\nNZ      27    104\n",
      "4     5        England\\nENG      24    101\n",
      "5     6    South Africa\\nSA      19    101\n",
      "6     7     Bangladesh\\nBAN      28     95\n",
      "7     8    Afghanistan\\nAFG      16     88\n",
      "8     9       Sri Lanka\\nSL      32     87\n",
      "9    10     West Indies\\nWI      38     68\n",
      "10   11       Zimbabwe\\nZIM      30     55\n",
      "11   12       Scotland\\nSCO      33     50\n",
      "12   13        Ireland\\nIRE      24     44\n",
      "13   14    Netherlands\\nNED      28     37\n",
      "14   15          Nepal\\nNEP      40     35\n",
      "15   16        Namibia\\nNAM      28     29\n",
      "16   17  United States\\nUSA      31     26\n",
      "17   18           Oman\\nOMA      24     22\n",
      "18   19            UAE\\nUAE      41     15\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "                                                 Rank                 Player  \\\n",
      "0                1\\n                        \\n\\n\\n(0)             Babar Azam   \n",
      "1        2\\n                                \\n\\n\\n(0)  Rassie van der Dussen   \n",
      "2        3\\n                                \\n\\n\\n(0)           Fakhar Zaman   \n",
      "3        4\\n                                \\n\\n\\n(0)            Imam-ul-Haq   \n",
      "4   5\\n                                \\n\\n\\n\\n\\n(...           Shubman Gill   \n",
      "..                                                ...                    ...   \n",
      "95      96\\n                                \\n\\n\\n(0)          Dasun Shanaka   \n",
      "96      97\\n                                \\n\\n\\n(0)       Avishka Fernando   \n",
      "97      98\\n                                \\n\\n\\n(0)              Ryan Burl   \n",
      "98      99\\n                                \\n\\n\\n(0)             Finn Allen   \n",
      "99     100\\n                                \\n\\n\\n(0)      Wanindu Hasaranga   \n",
      "\n",
      "   Team                          Rating  \n",
      "0   PAK   898 v West Indies, 10/06/2022  \n",
      "1    SA       796 v England, 19/07/2022  \n",
      "2   PAK   784 v New Zealand, 29/04/2023  \n",
      "3   PAK   815 v West Indies, 12/06/2022  \n",
      "4   IND   743 v West Indies, 01/08/2023  \n",
      "..  ...                             ...  \n",
      "95   SL         506 v India, 10/01/2023  \n",
      "96   SL  591 v South Africa, 02/09/2021  \n",
      "97  ZIM      437 v Scotland, 04/07/2023  \n",
      "98   NZ      500 v Pakistan, 09/01/2023  \n",
      "99   SL         433 v India, 12/01/2023  \n",
      "\n",
      "[100 rows x 4 columns]\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "                                                 Rank           Player Team  \\\n",
      "0                1\\n                        \\n\\n\\n(0)   Josh Hazlewood  AUS   \n",
      "1        2\\n                                \\n\\n\\n(0)   Mitchell Starc  AUS   \n",
      "2        3\\n                                \\n\\n\\n(0)      Rashid Khan  AFG   \n",
      "3        4\\n                                \\n\\n\\n(0)   Mohammed Siraj  IND   \n",
      "4        5\\n                                \\n\\n\\n(0)       Matt Henry   NZ   \n",
      "..                                                ...              ...  ...   \n",
      "95  96\\n                                \\n\\n\\n\\n\\n...    Henry Shipley   NZ   \n",
      "96  97\\n                                \\n\\n\\n\\n\\n...    Kasun Rajitha   SL   \n",
      "97      98\\n                                \\n\\n\\n(0)  Lalit Rajbanshi  NEP   \n",
      "98  99\\n                                \\n\\n\\n\\n\\n...   Ebadot Hossain  BAN   \n",
      "99  =\\n                                \\n\\n\\n\\n\\n\\...      Hamza Tahir  SCO   \n",
      "\n",
      "                           Rating  \n",
      "0       733 v England, 26/01/2018  \n",
      "1   783 v New Zealand, 29/03/2015  \n",
      "2      806 v Pakistan, 21/09/2018  \n",
      "3   736 v New Zealand, 21/01/2023  \n",
      "4    691 v Bangladesh, 26/03/2021  \n",
      "..                            ...  \n",
      "95     400 v Pakistan, 07/05/2023  \n",
      "96     411 v Scotland, 27/06/2023  \n",
      "97  397 v West Indies, 22/06/2023  \n",
      "98  389 v Afghanistan, 08/07/2023  \n",
      "99        484 v Nepal, 17/07/2022  \n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_icc_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'class': 'table'})\n",
    "        rows = table.find_all('tr')[1:]\n",
    "\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) == 4:  # Top 10 ODI teams\n",
    "                rank = columns[0].get_text().strip()\n",
    "                team = columns[1].get_text().strip()\n",
    "                matches = columns[2].get_text().strip()\n",
    "                points = columns[3].get_text().strip()\n",
    "                data.append([rank, team, matches, points])\n",
    "            elif len(columns) == 5:  # Top 10 ODI batsmen/bowlers\n",
    "                rank = columns[0].get_text().strip()\n",
    "                player = columns[1].get_text().strip()\n",
    "                team = columns[2].get_text().strip()\n",
    "                rating = columns[4].get_text().strip()\n",
    "                data.append([rank, player, team, rating])\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print('Failed to retrieve the page.')\n",
    "        return []\n",
    "\n",
    "# URLs for different ICC Cricket rankings\n",
    "odi_teams_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "odi_batsmen_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "odi_bowlers_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "\n",
    "odi_teams_data = scrape_icc_rankings(odi_teams_url)\n",
    "odi_batsmen_data = scrape_icc_rankings(odi_batsmen_url)\n",
    "odi_bowlers_data = scrape_icc_rankings(odi_bowlers_url)\n",
    "\n",
    "# Create DataFrames\n",
    "odi_teams_df = pd.DataFrame(odi_teams_data, columns=['Rank', 'Team', 'Matches', 'Points'])\n",
    "odi_batsmen_df = pd.DataFrame(odi_batsmen_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "odi_bowlers_df = pd.DataFrame(odi_bowlers_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "# Display DataFrames\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(odi_teams_df)\n",
    "\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(odi_batsmen_df)\n",
    "\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8384a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "4 columns passed, passed data had 5 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    981\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m   1029\u001b[0m             \u001b[1;31m# caller's responsibility to check for this...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m             raise AssertionError(\n\u001b[0m\u001b[0;32m   1031\u001b[0m                 \u001b[1;34mf\"{len(columns)} columns passed, passed data had \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 4 columns passed, passed data had 5 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4136\\3445162001.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# Create DataFrames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mwomen_odi_teams_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwomen_odi_teams_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Rank'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Team'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Matches'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Points'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mwomen_odi_batting_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwomen_odi_batting_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Rank'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Player'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Team'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mwomen_odi_all_rounder_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwomen_odi_all_rounder_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Rank'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Player'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Team'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    719\u001b[0m                         \u001b[1;31m# ndarray], Index, Series], Sequence[Any]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[0;32m    722\u001b[0m                         \u001b[1;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m                         \u001b[1;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 883\u001b[1;33m     \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_finalize_columns_and_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    884\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;31m# GH#26429 do not raise user-facing AssertionError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 4 columns passed, passed data had 5 columns"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_icc_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'class': 'table'})\n",
    "        rows = table.find_all('tr')[1:]\n",
    "\n",
    "        data = []\n",
    "        for row in rows:\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) == 5:  # Top 10 women's ODI teams\n",
    "                rank = columns[0].get_text().strip()\n",
    "                team = columns[1].get_text().strip()\n",
    "                matches = columns[2].get_text().strip()\n",
    "                points = columns[3].get_text().strip()\n",
    "                rating = columns[4].get_text().strip()\n",
    "                data.append([rank, team, matches, points, rating])\n",
    "            elif len(columns) == 6:  # Top 10 women's ODI batting players/all-rounders\n",
    "                rank = columns[0].get_text().strip()\n",
    "                player = columns[1].get_text().strip()\n",
    "                team = columns[2].get_text().strip()\n",
    "                rating = columns[5].get_text().strip()\n",
    "                data.append([rank, player, team, rating])\n",
    "\n",
    "        return data\n",
    "    else:\n",
    "        print('Failed to retrieve the page.')\n",
    "        return []\n",
    "\n",
    "# URLs for different ICC Cricket rankings\n",
    "women_odi_teams_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "women_odi_batting_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "women_odi_all_rounder_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "\n",
    "women_odi_teams_data = scrape_icc_rankings(women_odi_teams_url)\n",
    "women_odi_batting_data = scrape_icc_rankings(women_odi_batting_url)\n",
    "women_odi_all_rounder_data = scrape_icc_rankings(women_odi_all_rounder_url)\n",
    "\n",
    "# Create DataFrames\n",
    "women_odi_teams_df = pd.DataFrame(women_odi_teams_data, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "women_odi_batting_df = pd.DataFrame(women_odi_batting_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "women_odi_all_rounder_df = pd.DataFrame(women_odi_all_rounder_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "# Display DataFrames\n",
    "print(\"Top 10 Women's ODI Teams:\")\n",
    "print(women_odi_teams_df)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "print(women_odi_batting_df)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "print(women_odi_all_rounder_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ade2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the CNBC news page\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all news articles\n",
    "    news_articles = soup.find_all('li', class_='Card--cardWrapper--1vjc_2X9')\n",
    "    \n",
    "    # Initialize lists to store the data\n",
    "    headlines = []\n",
    "    times = []\n",
    "    news_links = []\n",
    "    \n",
    "    # Extract news details\n",
    "    for article in news_articles:\n",
    "        headline = article.find('h3', class_='Card--cardHeadline--2u4j5dUh').get_text().strip()\n",
    "        time = article.find('time', class_='Card--time--WwPkd3U4').get_text().strip()\n",
    "        news_link = 'https://www.cnbc.com' + article.find('a')['href']\n",
    "        headlines.append(headline)\n",
    "        times.append(time)\n",
    "        news_links.append(news_link)\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    news_df = pd.DataFrame({'Headline': headlines, 'Time': times, 'News Link': news_links})\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(news_df)\n",
    "else:\n",
    "    print('Failed to retrieve the page.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefe66a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the most downloaded articles page\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all article items\n",
    "    article_items = soup.find_all('div', class_='pod-listing')\n",
    "    \n",
    "    # Initialize lists to store the data\n",
    "    titles = []\n",
    "    authors_list = []\n",
    "    dates = []\n",
    "    paper_urls = []\n",
    "    \n",
    "    # Extract article details\n",
    "    for item in article_items:\n",
    "        title = item.find('a', class_='pod-listing-title').get_text().strip()\n",
    "        author_tags = item.find_all('a', class_='author-name')\n",
    "        authors = ', '.join([author.get_text().strip() for author in author_tags])\n",
    "        date = item.find('span', class_='pod-listing-date').get_text().strip()\n",
    "        paper_url = 'https://www.journals.elsevier.com' + item.find('a', class_='pod-listing-title')['href']\n",
    "        titles.append(title)\n",
    "        authors_list.append(authors)\n",
    "        dates.append(date)\n",
    "        paper_urls.append(paper_url)\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    articles_df = pd.DataFrame({'Paper Title': titles, 'Authors': authors_list, 'Published Date': dates, 'Paper URL': paper_urls})\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(articles_df)\n",
    "else:\n",
    "    print('Failed to retrieve the page.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e734d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the dineout.co.in page\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all restaurant cards\n",
    "    restaurant_cards = soup.find_all('div', class_='restnt-card-main')\n",
    "    \n",
    "    # Initialize lists to store the data\n",
    "    restaurant_names = []\n",
    "    cuisines = []\n",
    "    locations = []\n",
    "    ratings = []\n",
    "    image_urls = []\n",
    "    \n",
    "    # Extract restaurant details\n",
    "    for card in restaurant_cards:\n",
    "        name = card.find('div', class_='restnt-card-details')['title']\n",
    "        cuisine = card.find('p', class_='restnt-card-cuisine').get_text().strip()\n",
    "        location = card.find('p', class_='restnt-card-loc').get_text().strip()\n",
    "        rating = card.find('span', class_='restnt-card-rating').get_text().strip()\n",
    "        image_url = card.find('img', class_='restnt-card-thumbnail')['data-src']\n",
    "        restaurant_names.append(name)\n",
    "        cuisines.append(cuisine)\n",
    "        locations.append(location)\n",
    "        ratings.append(rating)\n",
    "        image_urls.append(image_url)\n",
    "    \n",
    "    # Create a DataFrame from the extracted data\n",
    "    restaurants_df = pd.DataFrame({\n",
    "        'Restaurant Name': restaurant_names,\n",
    "        'Cuisine': cuisines,\n",
    "        'Location': locations,\n",
    "        'Ratings': ratings,\n",
    "        'Image URL': image_urls\n",
    "    })\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(restaurants_df)\n",
    "else:\n",
    "    print('Failed to retrieve the page.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
